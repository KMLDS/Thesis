\chapter{Introduction}
% \pagestyle{fancy}
%\pagenumbering{arabic}
Occasionally in physics we are interested in measuring extremely small quantities.  If we are interested in pushing the limits of just how small an effect we would like to be able to measure, physics and mathematical statistics will place fundamental constraints on what can be accomplished.

To provide a motivating example which we will discuss at length over the course of this thesis, let's imagine an optical experiment measuring the tilt of a mirror simply by shining a laser source on the mirror and measuring the beam with a position sensing detector.  If we first consider the case where this experiment is performed under ideal conditions with no noise and a perfect detector, there are still limits on the minimum resolvable tilt.  Quantum mechanics imposes some uncertainty in the position distribution of photons coming from the source.  If the detector is composed of individual pixels, some information carried by the photons will be lost compared to the case of true continuous resolution.

As we will discuss further in this introductory chapter, the problem of finding the minimum resolvable tilt in this experiment is fully determined by the probability distribution of photons coming from the source, and a physical understanding of the parametric form the magnitude of the mirror tilt will have on the distribution.  Once that is known, it is straightforward to come up with a way to estimate the mirror tilt given some experimental data, and to determine the approximate error of that estimate.

Of course, no experiment can actually be carried out under noiseless conditions with perfect detectors.  On top of the quantum uncertainty discussed above, there will be a number of sources of technical noise.  In the simple mirror tilt experiment for instance, we would be likely to experience shifts in any of the experimental components due to vibrations, shifts in the beam due to turbulence, or electronic noise in the detector, to name a few.  With that in mind, it is also important to consider how robust a given measurement is to technical noise, and if there are ways to make it more robust to this noise.  As we will discuss throughout this work, a technique known as weak value amplification has proven capable of mitigating many forms of technical noise \cite{Jordan2014, Starling2009}.  This has been examined in detail both theoretically and experimentally since the discovery of weak values by Aharonov, Albert, and Vaidman in 1988 \cite{Aharonov1988}.

Another method of countering noise developed over the past several decades involves the use of nonclassical states of light.  There are numerous different nonclassical states capable of reducing the quantum uncertainty in some variable of interest, however the most prominent of these are the so-called squeezed states.  These allow the experimenter to decrease the variance in some variable at the expense of an increase in variance of the conjugate variable.  Squeezed states have been primarily considered in precision phase measurements, and in gravitational wave detection in particular \cite{Caves1981, Vahlbruch2005}.  They have been used to great effect in precision spatial measurements as well \cite{Barnett2003, Treps2002, Treps2003}, which we will focus on heavily in this work. 

In the remainder of this introductory chapter we will briefly discuss several physical and statistical methods and techniques that will be useful throughout the rest of the work.  This includes weak values, squeezed states of light, maximum likelihood estimation, and the Cram\'{e}r-Rao lower bound.  In chapter \ref{ch:pulsed}, we will discuss an enhancement to weak value amplification which recycles pulses of light to allow greatly improved measurement statistics.  This method is indeed an improvement, but requires precision electronics and active optics to function.  In chapter \ref{ch:cw}, this weakness is remedied by using a continuous wave recycling scheme which is entirely passive and actually allows for a larger enhancement over plain weak values than the active scheme.  In chapter \ref{ch:iwv}, we analyze the robustness of weak values to several types of technical noise and compare with more traditional approaches.  In chapter \ref{ch:squeezed} we analyze a squeezed light scheme which combines nonclassical light and weak values.  This shows that the two are not mutually exclusive, and it is in fact possible to mitigate the effects of technical noise and quantum uncertainty simultaneously.  Finally, in chapter \ref{ch:biphotons} we discuss an alternative optical displacement measurement using correlated biphoton pairs.  We show that even if the uncertainty in the individual photons is large, it is possible to make the uncertainty in the sum of the positions of the two photons small, which in turn can be used to improve measurement sensitivity.  

\section{Weak values}
In a typical quantum measurement scheme, we have some quantum system of interest which we entangle with a pointer which is measured at a detector.  We take $\op{A}$ to be a some system quantum variable, and $\op{x}$ to be a pointer quantum variable.  For simplicity, we also take the initial state of the system and pointer to be pure, \emph{i.e.,}
\begin{align}
  \ket{\Psi_0} &= \ket{\psi_0} \otimes \ket{\varphi_0}.
\end{align}
Finally, the interaction Hamiltonian combining the system and meter is given by
\begin{align}
  \op{H} &= \epsilon \op{A} \otimes \op{x},
\end{align}
which modifies the state as
\begin{align}
\nonumber  \ket{\Psi} &= \op{U}\ket{\Psi_0}, \\
                      &= \exp\left(-i\epsilon \op{A} \otimes \op{x} \right) \ket{\Psi_0}.
\end{align}
Here we are restricting our discussion to initial product states and time independent Hamiltonians for clarity in this short introduction.  It is straightforward to remove either restriction, and doing so doesn't change anything fundamental about our discussion of quantum measurements and weak values.  For a more in depth review of both theory and applications, see the review article \cite{Dressel2014}.

In most cases, we imagine the experimenter makes a direct measurement on the pointer state after this interaction takes place.  In a weak value measurement however, an intermediate step is taken first.  We make a measurement on the system state, in a process called postselection.  That is, we choose a postselection state $\ket{\psi_{ps}}$, and make a projective measurement on the state $\ket{\Psi}$,
\begin{align}\label{eq:postselection state}
  \nonumber \frac{\pr{\psi_{ps}}}{\ipr{\psi_{ps}}{\psi_0}} \ket{\Psi} &= \frac{\bra{\psi_{ps}}\op{U}\ket{\psi_0}}{\ipr{\psi_{ps}}{\psi_0}}\ket{\psi_{ps}} \otimes \ket{\varphi_0}, \\
                                                                      &= \left(1 - i\epsilon A_w \op{x} + ...  \right)\ket{\psi_{ps}} \otimes \ket{\varphi_0},
\end{align}
where $A_w$ is the so-called ``weak value'' given by
\begin{align}
  A_w &= \frac{\bra{\psi_{ps}}\op{A}\ket{\psi_0}}{\ipr{\psi_{ps}}{\psi_0}}.
\end{align}
We have truncated the expansion \eqref{eq:postselection state} to first order in $\epsilon$ for clarity only, the weak value effect is still well defined even if higher order terms are not small.  Throughout this work we will be principally concerned with very small effects, and this first order approximation is well justified for our systems of interest.  The shift in the average value of the pointer variable is then
\begin{align}
  \bra{\Psi_0}\op{X}\ket{\Psi_0} - \bra{\Psi_{ps}}\op{X}\ket{\Psi_{ps}} &= 2i\epsilon ~\text{Im}{A_w} \bra{\varphi_0}\op{X}^2\ket{\varphi_0} + ...
\end{align}
In the limit of very small $\ipr{\psi_{ps}}{\psi_0}$, the weak value can be made arbitrarily large, and so this pointer shift can be made very large as well.  Note however that $|\ipr{\psi_{ps}}{\psi_0}|^2$ gives the postselection probability (i.e.~the probability of a given event being postselected), so fewer probes make it to the detector for larger pointer shifts.  In this work we will only be concerned with weak value measurements that have a nonzero imaginary compoAs we will discuss in later chapters, this is the essential tradeoff in weak value measurements, and it actually prevents weak value measurements from exceeding the sensitivity of other types of measurements in the absence of noise.

Despite this, weak values have proven remarkably effective in a number of experiments to date.  This is primarily due to weak value measurements being robust to many commonly encountered forms of techincal noise.  We have also showed theoretically that weak value measurements can be ``recycled'' to allow probes which fail postselection to be reused, in principle allowing all probes to be eventually recycled.  This effect has been recently demonstrated experimentally.

\section{Squeezed light}
While weak values are very useful in reducing technical noise, they do not allow one to reduce quantum uncertainty while using classical (\emph{i.e.,} coherent) light.  It is possible to do this however by using nonclassical light.  Here we will treat the specific case of single mode quadrature squeezed light for simplicity, leaving the case of spatial multimode squeezing for chapter~\ref{ch:squeezed} and the case of an alternate choice of nonclassical light in chapter~\ref{ch:biphotons}.  This section will largely follow the treatment of squeezed states by \cite{Agarwal2013}, other useful general references include \cite{Mandel1995, Loudon2000}.

We begin by defining the so-called ``squeezed vacuum state''
\begin{align}
  \ket{\xi} &= \op{S}(\xi)\ket{0},
\end{align}
where $\op{S}(\xi)$ is the unitary ``squeeze operator'' defined by
\begin{align}
  \op{S}(\xi) &= \exp\left[\frac{1}{2}\left(\xi\op{a}^{\dagger 2} - \xi^{*}\op{a}^2 \right) \right].
\end{align}
We will now show the field quadratures $\op{X}$ and $\op{Y}$ defined as
\begin{align}\label{eq:quadratures}
  \op{X} &= \frac{1}{\sqrt{2}} \left(\annihilation{a}{} + \creation{a}{} \right), \nonumber \\
  \op{Y} &= \frac{1}{\sqrt{2i}} \left(\annihilation{a}{} - \creation{a}{} \right),
\end{align}
can be squeezed, in the sense that the variance of one can be reduced at the cost of raising the variance of the other.

For this task, it is convenient to work with the Bogoliubov transformed operator
\begin{align}
  \annihilation{a}{}(\xi) &= \op{S}^\dagger \annihilation{a}{} \op{S}, \nonumber \\
                          &= \annihilation{a}{} \cosh r + \creation{a}{} e^{i\theta}\sinh r,
\end{align}
and the transformed creation operator given by the Hermitian conjugate.  Here $r$ is the real part of the squeeze parameter $\xi$ and $\theta$ is $\arg{\xi}$.  This allows us to convert problems using field variables on the squeezed vacuum state to ones using transformed variables on the vacuum.  As a very simple example,
\begin{align}
  \mean{\numberop{a}{}} &= \bra{\xi}\numberop{a}{}\ket{\xi}, \nonumber \\
                            &= \bra{0}\op{S}^\dagger \creation{a}{} \op{S}\op{S}^\dagger \annihilation{a}{} \op{S} \ket{0}, \nonumber \\
                            &= \bra{0}\creation{a}{}(\xi) \annihilation{a}{}(\xi)\ket{0}.
\end{align}
Using this method to calculate the first two moments of the quadratures \eqref{eq:quadratures} in the squeezed vacuum state, a somewhat lengthy calculation yields
\begin{align}
  \var{\op{X}} &= \frac{1}{2}e^{-2r}, \nonumber \\
  \var{\op{Y}} &= \frac{1}{2}e^{2r},
\end{align}
for an appropriate choice of phase.  Hence, the variance of one quadrature can be made arbitrarily small at the cost of making the variance of the other arbitrarily large.  This effect is not limited to the quadratures of the field however, and we will see a completely analogous variance reduction to spatial measurements of an optical beam in chapter \ref{ch:squeezed}.

\section{Statistical methods}
\subsection{Signal to noise ratio}
The simplest statistic we will use in this work is the signal to noise ratio $\mathcal{R}$, used frequently in the literature \cite{Dixon2009, Loudon2000, Starling2009}.  If we have some experimental data $\{X_1, X_2,..., X_n\}$, the signal to noise is given by
\begin{align}
  \nonumber  \mathcal{R} &= \frac{\mean{X}}{\sqrt{\var{X}}}, \\
                         &\approx \frac{\mean{X}}{\sqrt{\mean{X^2}}}, 
\end{align}
where $\mean{X}$ is assumed to be very close to zero due to the smallness of the parameter we are making inferences about.  We then make the argument that $\mean{X} \sim \sqrt{\var{X}}$ sets the minimum resolvable signal, i.e.~$x_{min} \approx \sqrt{\var{X}}$.  

Note this metric is closely related to Student's $t$ statistic commonly used by statisticians \cite{Casella2002},
\begin{align}
  t = \frac{\mean{X} - \mu}{\sqrt{S_n}/n},
\end{align}
where $\mean{X}$ is the sample mean, $\mu$ is the population mean, and $S_n$ is the sample variance.  In most cases we cannot know the population mean $\mu$, so $\mathcal{R}$ will follow a $t$ distribution shifted by some unknown amount and scaled by the number of events.  This is useful for hypothesis testing, \emph{e.g.~}if we wanted to test whether the true population mean is zero.  The unknown shift limits the usefulness of signal to noise for making formal statements about the estimates of the parameter.  It still finds widespread use however in the literature due to being an intuititive estimator that usually has reliable performance for reason we will discuss in the next section.  

\subsection{Parameter estimation}
Typically in a precision measurement of a physical quantity, we are able to fully determine the parametric form of the sampling distribution of our data from physical first principles.  This makes precision physical measurements an ideal use case for the statistical method of maximum likelihood estimation.  Again having some data $\{X_1, X_2,..., X_n\}$, the likelihood function is defined as
\begin{align}\label{eq:likelihood function}
  L(\theta) = \prod^n_i p(X=x_i|\theta),
\end{align}
where $\theta$ is the parameter of interest in the single parameter case, or a vector of parameters in the multivariate case \cite{Casella2002, Wasserman2004}.  We will restrict our attention to the univariate case throughout this work.  Note that the likelihood function is defined in terms of fixed data, and has only functional dependence on the parameter $\theta$.

To obtain a maximum likelihood estimate (MLE), we simply take a derivative of the likelihood function with respect to $\theta$ and set it equal to zero.  Solving the resulting equation provides the maximum likelihood estimate.  Since it is cumbersome to solve $\partial L/ \partial \theta = 0$ in most cases of interest, it is much more common to work with the log-likelihood,
\begin{align}
\nonumber  \ell(\theta) &= \log L(\theta), \\
                        &=  \sum^n_i  \log p(X=x_i|\theta),
\end{align}
which has the same argmax as \eqref{eq:likelihood function} since $\log(x)$ is a monotonically increasing function of $x$.

The maximum likelihood estimator provides a very useful way to estimate the value of some parameter.  It is also possible in this framework to get reliable estimates of the expected errors of the estimators.  To begin, we will introduce a quantity known as Fisher information \cite{Casella2002, Wasserman2004},
\begin{align}
\nonumber  \mathcal{I}(\theta) &= -\mathbb{E}\left[\partial^2_\theta \ln p(x|\theta)\right], \\
                               &= -\int \text{d}x~ p(x|\theta)\partial^2_\theta \ln p(x|\theta).
\end{align}
For any unbiased estimator $\hat{\theta}$ (\emph{i.e.~} $\mathbb{E}[\hat{\theta}] = \theta$), the variance of the estimator obeys the Cram\'{e}r-Rao lower bound,
\begin{align}
  \var{\hat{\theta}} \ge \frac{1}{\mathcal{I}(\theta)}.
\end{align}
If a given estimator saturates the inequality it is said to be ``efficient''.  Most maximum likelihood estimators we will encounter are efficient, and even in cases where they are not, the Cram\'{e}r-Rao inequality gives a good approximate value of the variance.  This in turn allows for a straightforward way to construct confidence intervals or make signal-to-noise type arguments like $\theta_{min} \sim \mathcal{I}(\theta)^{-1/2}$.

\subsection{A simple example}
It is helpful to consider a maximum likelihood estimation example to clarify why it is useful in practice.  Say we have calculated the distribution of an optical beam just before a position sensing detector will be of the form
\begin{align}
  p(x|\theta) &= \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{(x-\theta)^2}{2} \right),
\end{align}
where $\theta$ is an unknown displacement of the beam and we are working in units where the variance in position of the beam is one.

If $n$ photons are measured at positions $x_i$, the log-likelihood is given by
\begin{align}
  \ell(\theta) &= -\sum_i^n \frac{(x_i - \theta)}{2} + \text{const}.
\end{align}
To find the maximum likelihood estimator we take the derivative of this expression with respect to $\theta$ and set it equal to zero,
\begin{align}
\nonumber  \frac{\partial \ell}{\partial \theta} &= \sum_i^n (x_i - \theta), \\
\end{align}
which yields
\begin{align}
  \hat{\theta} &= \frac{1}{n}\sum_i^n x_i.
\end{align}
That is, the maximum likelihood estimate of $\theta$ is equal to the sample average of our data.  To obtain the variance in the estimator we simply calculate the Fisher information
\begin{align}
\nonumber  \mathcal{I}(\theta) &= -\int_{-\infty}^{\infty}\text{d}x~ p(x|\theta) \partial_\theta^2  p(x|\theta), \\
\nonumber                      &= \int_{-\infty}^{\infty}\text{d}x~ p(x|\theta), \\
                               &= 1.
\end{align}
Hence, the Fisher information is equal to the inverse spatial variance (also numerically equal to the variance in this case) of the beam at the detector.  This tells us the minimum resolvable parameter is approximately given by $\theta_{min} \sim 1$ and an approximate 95\% confidence interval is given by $(\mean{X} - 2, \mean{X} + 2)$.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
